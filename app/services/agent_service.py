#!/usr/bin/env python3
"""
ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤
ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œê³¼ ì„¸ì…˜ ê´€ë¦¬ìë¥¼ ë˜í•‘í•©ë‹ˆë‹¤.
"""

import sys
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime
import time
import uuid

# backend/agent ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
backend_agent_path = str(Path(__file__).parent.parent.parent / "backend" / "agent")
if backend_agent_path not in sys.path:
    sys.path.insert(0, backend_agent_path)

from app.core.logging_config import get_logger
from app.core.config import get_settings
from app.models.chat_models import get_model_by_name, get_default_model
from app.services.storm_service import StormService

logger = get_logger("agent_service")

# ë°±ì—”ë“œ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì„í¬íŠ¸
BACKEND_AVAILABLE = False
try:
    import multi_agent_system
    from multi_agent_system import get_magentic_system, KTDSMagenticOneSystem
    from session_manager import SessionManager as BackendSessionManager
    from conversation_context import ConversationManager
    BACKEND_AVAILABLE = True
    logger.info("âœ… Backend multi-agent system imported successfully")
except ImportError as e:
    logger.warning(f"âš ï¸ Backend system not available: {e}")
    BACKEND_AVAILABLE = False
except Exception as e:
    logger.error(f"âŒ Unexpected error importing backend system: {e}")
    BACKEND_AVAILABLE = False

class AgentService:
    """
    ë©€í‹°ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    ConversationManager ì¤‘ì‹¬ì˜ í†µí•© ì„¸ì…˜ ê´€ë¦¬ ë° STORM ì—ì´ì „íŠ¸ ì§€ì›
    """
    
    def __init__(self):
        self.settings = get_settings()
        self.initialized = False
        self.multi_agent_system = None
        self.conversation_manager = None
        self.session_manager = None  # ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, í•˜ì§€ë§Œ ìµœì†Œ ì‚¬ìš©
        self.storm_service = None    # STORM ì„œë¹„ìŠ¤ ì¶”ê°€
        self.agent_count = 0

    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if self.initialized:
            logger.info("Agent Service already initialized")
            return
        
        try:
            logger.info("Initializing Agent Service...")
            
            # ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            await self._initialize_multi_agent_system()
            
            # ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ìµœì†Œ SessionManager ì´ˆê¸°í™”
            await self._initialize_minimal_session_manager()
            
            # ConversationManager ì´ˆê¸°í™” (ë©”ì¸ ê´€ë¦¬ì)
            await self._initialize_conversation_manager()
            
            # STORM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            await self._initialize_storm_service()
            
            self.initialized = True
            logger.info("Agent Service initialized successfully")
            
        except Exception as e:
            logger.error("Failed to initialize Agent Service", error=str(e))
            raise

    async def _initialize_multi_agent_system(self, model_name: str = "gpt-4.1"):
        """ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if BACKEND_AVAILABLE:
                logger.info(f"ğŸš€ Initializing KTDS MagenticOne System with model: {model_name}...")
                
                self.multi_agent_system = get_magentic_system(verbose=True, model_name=model_name)
                logger.info("âœ… MagenticOne system initialized successfully")
                
                if self.multi_agent_system:
                    # ì—ì´ì „íŠ¸ ê°œìˆ˜ ì„¤ì •
                    self.agent_count = getattr(self.multi_agent_system, 'agent_count', 0)
                    logger.info("âœ… KTDS MagenticOne system ready",
                              agent_count=self.agent_count,
                              architecture=getattr(self.multi_agent_system, 'architecture', []),
                              model=getattr(self.multi_agent_system, 'model_name', 'gpt-4'),
                              system_name=getattr(self.multi_agent_system, 'system_name', 'KTDS MagenticOne'),
                              total_tools=getattr(self.multi_agent_system, 'total_tools', 0))
                
        except Exception as e:
            logger.error("Failed to initialize multi-agent system", error=str(e))
            self.multi_agent_system = None

    async def _initialize_minimal_session_manager(self):
        """ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ìµœì†Œ SessionManager (ConversationManagerê°€ ë©”ì¸)"""
        try:
            if BACKEND_AVAILABLE:
                logger.info("Initializing minimal Session Manager for compatibility...")
                
                # SessionManagerëŠ” ì´ì œ ìµœì†Œ ê¸°ëŠ¥ë§Œ ì œê³µ
                self.session_manager = BackendSessionManager()
                logger.info("âœ… Minimal Session Manager initialized successfully")
                
                if self.session_manager:
                    stats = self.session_manager.get_session_stats()
                    logger.info("Session Manager ready",
                              active_sessions=stats.get("total_active_sessions", 0),
                              manager_type=stats.get("manager_type", "unknown"))
                              
        except Exception as e:
            logger.error("Failed to initialize session manager", error=str(e))
            self.session_manager = None
    
    async def _initialize_conversation_manager(self):
        """ConversationManager ì´ˆê¸°í™” (ë©”ì¸ ì„¸ì…˜ ê´€ë¦¬ì)"""
        try:
            if BACKEND_AVAILABLE:
                logger.info("Initializing Conversation Manager (Main Session Manager)...")
                
                # KTDS ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
                system_message = """ë‹¹ì‹ ì€ KTDS(í•œêµ­ê¸°ì—…ë°ì´í„°)ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ë©°, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì—°ê´€ëœ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
íšŒì‚¬ ì •ë³´, ê¸°ìˆ  ì§€ì›, ì¼ë°˜ì ì¸ ì—…ë¬´ ë¬¸ì˜ì— ì „ë¬¸ì ìœ¼ë¡œ ëŒ€ì‘í•©ë‹ˆë‹¤."""
                
                self.conversation_manager = ConversationManager(
                    default_buffer_size=20,  # ìµœê·¼ 20ê°œ ë©”ì‹œì§€ ìœ ì§€
                    default_system_message=system_message,
                    session_manager=self.session_manager  # Azure SQL ì €ì¥ ì§€ì›
                )
                logger.info("âœ… Conversation Manager initialized successfully")
                
                if self.conversation_manager:
                    stats = self.conversation_manager.get_session_stats()
                    logger.info("Conversation Manager ready",
                              active_sessions=stats.get("total_active_sessions", 0),
                              buffer_size=stats.get("default_buffer_size", 0))
                              
        except Exception as e:
            logger.error("Failed to initialize conversation manager", error=str(e))
            self.conversation_manager = None

    async def _initialize_storm_service(self):
        """STORM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            logger.info("Initializing STORM Service...")
            
            self.storm_service = StormService()
            await self.storm_service.initialize()
            
            logger.info("âœ… STORM Service initialized successfully")
            
        except Exception as e:
            logger.error("Failed to initialize STORM service", error=str(e))
            self.storm_service = None

    def _create_model_client(self, model_name: Optional[str] = None):
        """ë™ì  ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        try:
            # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            if model_name:
                model_config = get_model_by_name(model_name)
                if not model_config:
                    logger.warning(f"Model {model_name} not found, using default")
                    model_config = get_default_model()
            else:
                model_config = get_default_model()
            
            logger.info(f"Creating model client for {model_config.name}")
            
            # í”„ë¡œë°”ì´ë”ë³„ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            if model_config.provider == "azure_openai":
                from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
                
                return AzureOpenAIChatCompletionClient(
                    model=model_config.name,
                    api_key=model_config.api_key,
                    azure_endpoint=model_config.endpoint,
                    api_version=model_config.api_version,
                    temperature=model_config.temperature
                )
            
            elif model_config.provider == "gemini":
                # Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)
                logger.warning("Gemini provider not yet implemented, using default Azure OpenAI")
                default_config = get_default_model()
                from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
                
                return AzureOpenAIChatCompletionClient(
                    model=default_config.name,
                    api_key=default_config.api_key,
                    azure_endpoint=default_config.endpoint,
                    api_version=default_config.api_version,
                    temperature=default_config.temperature
                )
            
            elif model_config.provider == "friendli":
                # Friendli í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)
                logger.warning("Friendli provider not yet implemented, using default Azure OpenAI")
                default_config = get_default_model()
                from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
                
                return AzureOpenAIChatCompletionClient(
                    model=default_config.name,
                    api_key=default_config.api_key,
                    azure_endpoint=default_config.endpoint,
                    api_version=default_config.api_version,
                    temperature=default_config.temperature
                )
            
            else:
                logger.error(f"Unsupported provider: {model_config.provider}")
                raise ValueError(f"Unsupported provider: {model_config.provider}")
                
        except Exception as e:
            logger.error(f"Failed to create model client: {str(e)}")
            # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            default_config = get_default_model()
            from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
            
            return AzureOpenAIChatCompletionClient(
                model=default_config.name,
                api_key=default_config.api_key,
                azure_endpoint=default_config.endpoint,
                api_version=default_config.api_version,
                temperature=default_config.temperature
            )

    async def process_message(
        self,
        message: str,
        user_id: str,
        session_id: str,
        context: Optional[Dict[str, Any]] = None,
        agent_mode: str = "normal",
        model_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì²˜ë¦¬ (ConversationManager ì¤‘ì‹¬ + STORM ëª¨ë“œ ì§€ì› + ëŒ€í™”í˜• ì„¤ì • + STORM ìƒí˜¸ì‘ìš©)
        """
        if not self.initialized:
            await self.initialize()
        
        # ëª¨ë¸ì´ ì§€ì •ëœ ê²½ìš° ìƒˆë¡œìš´ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if model_name and model_name != getattr(self.multi_agent_system, 'model_name', None):
            logger.info(f"ğŸ”„ Switching model to: {model_name}")
            await self._initialize_multi_agent_system(model_name)
        
        try:
            logger.info("Processing message",
                       message_length=len(message),
                       session_id=session_id,
                       user_id=user_id,
                       agent_mode=agent_mode)
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            conv_context = None
            if self.conversation_manager:
                conv_context = self.conversation_manager.get_or_create_context(session_id)
            
            # STORM ìƒí˜¸ì‘ìš© ì²˜ë¦¬ (ì§„í–‰ ì¤‘ì¸ STORM ì—°êµ¬ì˜ Human-in-the-Loop ì‘ë‹µ)
            if agent_mode == "normal" and conv_context and self.storm_service:
                # ì§„í–‰ ì¤‘ì¸ STORM ìƒí˜¸ì‘ìš© í™•ì¸
                if session_id in self.storm_service.human_interactions:
                    interaction = self.storm_service.human_interactions[session_id]
                    
                    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                    conv_context.add_user_message(
                        content=message,
                        source=user_id,
                        metadata=context
                    )
                    
                    # ì‘ë‹µ ì²˜ë¦¬
                    response_processed = await self._process_storm_interaction_response(
                        session_id, interaction, message, conv_context
                    )
                    
                    if response_processed:
                        return response_processed
            
            # STORM ëŒ€í™”í˜• ì„¤ì • ì²˜ë¦¬
            if agent_mode == "normal" and conv_context:
                # ì´ì „ ëŒ€í™”ì—ì„œ STORM ì—°êµ¬ì ìˆ˜ë¥¼ ë¬¼ì–´ë´¤ëŠ”ì§€ í™•ì¸
                last_messages = conv_context.get_recent_messages(2)
                if last_messages:
                    last_assistant_message = None
                    for msg in reversed(last_messages):
                        # msgê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ ê°ì²´ì¸ì§€ í™•ì¸
                        if isinstance(msg, dict):
                            role = msg.get("role", "")
                            content = msg.get("content", "")
                        else:
                            role = getattr(msg, 'role', '')
                            if hasattr(role, 'value'):
                                role = role.value
                            content = getattr(msg, 'content', '')
                        
                        if role == "assistant":
                            last_assistant_message = content
                            break
                    
                    # ì—°êµ¬ì ìˆ˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì¸ì§€ í™•ì¸
                    if (last_assistant_message and 
                        ("ëª‡ ëª…ì˜ ì—°êµ¬ì" in last_assistant_message or "ëª‡ ëª…ìœ¼ë¡œ" in last_assistant_message) and
                        message.strip().isdigit()):
                        
                        editor_count = int(message.strip())
                        if 1 <= editor_count <= 8:
                            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                            conv_context.add_user_message(
                                content=message,
                                source=user_id,
                                metadata=context
                            )
                            
                            # STORM ì£¼ì œ ì°¾ê¸° (ìµœê·¼ ë©”ì‹œì§€ì—ì„œ)
                            storm_topic = self._extract_storm_topic_from_history(last_messages)
                            
                            # ëŒ€í™”í˜• STORM ì—°êµ¬ ì‹œì‘
                            if storm_topic:
                                return await self._start_conversational_storm_research(
                                    storm_topic, editor_count, user_id, session_id, conv_context
                                )
                        else:
                            # ì˜ëª»ëœ ìˆ«ì ë²”ìœ„
                            error_response = f"âŒ ì—°êµ¬ì ìˆ˜ëŠ” 1ëª…ì—ì„œ 8ëª… ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ì…ë ¥: {editor_count}ëª…\n\në‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš” (1-8):"
                            conv_context.add_user_message(content=message, source=user_id, metadata=context)
                            conv_context.add_assistant_message(content=error_response, source="STORM_Setup_Agent", metadata={"storm_setup_error": True})
                            
                            return {
                                "response": error_response,
                                "session_id": session_id,
                                "agent_name": "STORM_Setup_Agent",
                                "agent_type": "storm",
                                "agent_description": "STORM ì„¤ì • ì—ì´ì „íŠ¸",
                                "confidence": 0.9,
                                "tools_used": [],
                                "execution_time": 0.0,
                                "metadata": {"storm_setup_error": True}
                            }
                
                # STORM ì—°êµ¬ í‚¤ì›Œë“œ ê°ì§€ (ìƒˆë¡œìš´ ìš”ì²­)
                storm_keywords = ["ì—°êµ¬í•´ì¤˜", "ì—°êµ¬í•˜ê³ ", "ì—°êµ¬ í•´ì¤˜", "ë¦¬ì„œì¹˜", "ì¡°ì‚¬í•´ì¤˜", "ë¶„ì„í•´ì¤˜", "ì •ë¦¬í•´ì¤˜", "ì¨ì¤˜", "ì‘ì„±í•´ì¤˜"]
                if any(keyword in message.lower() for keyword in storm_keywords):
                    # ì£¼ì œ ì¶”ì¶œ
                    topic = self._extract_topic_from_message(message)
                    
                    if topic:
                        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        conv_context.add_user_message(
                            content=message,
                            source=user_id,
                            metadata=context
                        )
                        
                        # ì—°êµ¬ì ìˆ˜ ì§ˆë¬¸
                        question_response = f"ğŸ¯ **STORM ì—°êµ¬ ëª¨ë“œ ê°ì§€**\n\n" + \
                                          f"**ì£¼ì œ**: {topic}\n\n" + \
                                          f"ëª‡ ëª…ì˜ ì—°êµ¬ìë¡œ ì§„í–‰í• ê¹Œìš”? (1-8ëª…)\n" + \
                                          f"ì˜ˆ: 3, 5, 7"
                        
                        # ì§ˆë¬¸ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        conv_context.add_assistant_message(
                            content=question_response,
                            source="STORM_Setup_Agent",
                            metadata={"storm_setup_question": True, "storm_topic": topic}
                        )
                        
                        return {
                            "response": question_response,
                            "session_id": session_id,
                            "agent_name": "STORM_Setup_Agent",
                            "agent_type": "storm",
                            "agent_description": "STORM ì„¤ì • ì—ì´ì „íŠ¸",
                            "confidence": 0.9,
                            "tools_used": [],
                            "execution_time": 0.0,
                            "metadata": {"storm_setup_question": True, "storm_topic": topic}
                        }
            
            # ê¸°ì¡´ STORM ëª¨ë“œ ì²˜ë¦¬ (ì§ì ‘ í˜¸ì¶œ)
            if agent_mode == "storm":
                if not self.storm_service:
                    raise ValueError("STORM service not available")
                
                # ì»¨í…ìŠ¤íŠ¸ì—ì„œ STORM ì„¤ì • ì¶”ì¶œ
                storm_config = None
                if context and "storm_config" in context:
                    from app.models.chat_models import StormConfig
                    storm_config = StormConfig(**context["storm_config"])
                
                return await self.storm_service.process_storm_research(
                    topic=message,
                    user_id=user_id,
                    session_id=session_id,
                    config=storm_config
                )
            
            # ì¼ë°˜ ëª¨ë“œ ì²˜ë¦¬
            if not self.multi_agent_system:
                raise ValueError("Multi-agent system not available")
            
            # ConversationManagerë¡œ ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬
            result = await self._process_with_conversation_manager(
                message, user_id, session_id, context
            )
            
            return result
            
        except Exception as e:
            logger.error("Failed to process message",
                        error=str(e),
                        session_id=session_id,
                        user_id=user_id,
                        agent_mode=agent_mode)
            raise

    async def _start_conversational_storm_research(
        self,
        storm_topic: str,
        editor_count: int,
        user_id: str,
        session_id: str,
        conv_context
    ) -> Dict[str, Any]:
        """
        ëŒ€í™”í˜• STORM ì—°êµ¬ ì‹œì‘ (Human-in-the-Loop ì§€ì›)
        """
        from app.models.chat_models import StormConfig
        
        # STORM ì„¤ì • ìƒì„±
        storm_config = StormConfig(
            enabled=True,
            editor_count=editor_count,
            human_in_the_loop=True,
            enable_search=True,
            enable_interviews=True,
            enable_realtime_progress=True
        )
        
        # ì‹œì‘ ë©”ì‹œì§€ ìƒì„±
        start_response = f"ğŸŒ©ï¸ **STORM ì—°êµ¬ ì‹œì‘**\n\n" + \
                        f"**ì£¼ì œ**: {storm_topic}\n" + \
                        f"**ì—°êµ¬ì ìˆ˜**: {editor_count}ëª…\n" + \
                        f"**ëª¨ë“œ**: Human-in-the-Loop í™œì„±í™”\n\n" + \
                        f"ì—°êµ¬ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤...\n\n" + \
                        f"ğŸ’¡ **ì•ˆë‚´**: ì—°êµ¬ ê³¼ì •ì—ì„œ ì•„ì›ƒë¼ì¸ ê²€í† , í¸ì§‘ì ìŠ¹ì¸ ë“±ì˜ ë‹¨ê³„ë§ˆë‹¤ í™•ì¸ì„ ìš”ì²­í•  ì˜ˆì •ì…ë‹ˆë‹¤."
        
        # ì‹œì‘ ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        conv_context.add_assistant_message(
            content=start_response,
            source="STORM_Setup_Agent",
            metadata={"storm_setup": True, "storm_topic": storm_topic}
        )
        
        try:
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ STORM ì—°êµ¬ ì‹œì‘
            asyncio.create_task(self._run_background_storm_research(
                storm_topic, editor_count, user_id, session_id, storm_config
            ))
            
            return {
                "response": start_response,
                "session_id": session_id,
                "agent_name": "STORM_Setup_Agent",
                "agent_type": "storm",
                "agent_description": "ëŒ€í™”í˜• STORM ì—°êµ¬ ì‹œì‘",
                "confidence": 0.95,
                "tools_used": ["storm_initialization"],
                "execution_time": 0.0,
                "metadata": {
                    "storm_setup": True,
                    "storm_topic": storm_topic,
                    "editor_count": editor_count,
                    "human_in_the_loop": True
                }
            }
            
        except Exception as e:
            logger.error("Failed to start conversational STORM research", error=str(e))
            raise

    async def _run_background_storm_research(
        self,
        topic: str,
        editor_count: int,
        user_id: str,
        session_id: str,
        storm_config
    ):
        """
        ë°±ê·¸ë¼ìš´ë“œì—ì„œ STORM ì—°êµ¬ ì‹¤í–‰ (Human-in-the-Loop ì§€ì›)
        """
        try:
            # ì»¤ìŠ¤í…€ STORM ì‹œìŠ¤í…œ ìƒì„±
            from backend.agent.storm_research import ParallelStormResearchSystem
            
            custom_storm = ParallelStormResearchSystem(max_workers=editor_count)
            
            # ëŒ€í™”í˜• ì½œë°± ì„¤ì •
            custom_storm.human_interaction_callback = lambda interaction_type, content, options=None: asyncio.create_task(
                self._handle_conversational_storm_interaction(
                    session_id, interaction_type, content, options
                )
            )
            
            # STORM ì—°êµ¬ ì‹¤í–‰
            result = await custom_storm.run_parallel_storm_research(
                topic=topic,
                enable_human_loop=True
            )
            
            # ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            if self.conversation_manager:
                conv_context = self.conversation_manager.get_or_create_context(session_id)
                completion_message = f"âœ… **STORM ì—°êµ¬ ì™„ë£Œ**\n\n{result}"
                conv_context.add_assistant_message(
                    content=completion_message,
                    source="STORM_Research_Agent",
                    metadata={"storm_completed": True}
                )
            
        except Exception as e:
            logger.error("Background STORM research failed", error=str(e))
            # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            if self.conversation_manager:
                conv_context = self.conversation_manager.get_or_create_context(session_id)
                error_message = f"âŒ **STORM ì—°êµ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ**\n\n{str(e)}"
                conv_context.add_assistant_message(
                    content=error_message,
                    source="STORM_Research_Agent",
                    metadata={"storm_error": True}
                )

    async def _handle_conversational_storm_interaction(
        self,
        session_id: str,
        interaction_type: str,
        content: str,
        options: List[str] = None
    ):
        """
        ëŒ€í™”í˜• STORM ìƒí˜¸ì‘ìš© ì²˜ë¦¬
        """
        try:
            # ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ STORM ì„œë¹„ìŠ¤ì— ë“±ë¡
            interaction_id = str(uuid.uuid4())
            
            if not hasattr(self.storm_service, 'human_interactions'):
                self.storm_service.human_interactions = {}
            
            self.storm_service.human_interactions[session_id] = {
                "interaction_id": interaction_id,
                "type": interaction_type,
                "content": content,
                "options": options or [],
                "timestamp": time.time(),
                "sent": False,
                "response": None
            }
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ìƒí˜¸ì‘ìš© ë©”ì‹œì§€ ì¶”ê°€
            if self.conversation_manager:
                conv_context = self.conversation_manager.get_or_create_context(session_id)
                
                # ìƒí˜¸ì‘ìš© ìœ í˜•ë³„ ë©”ì‹œì§€ ìƒì„±
                if interaction_type == "editor_review":
                    interaction_message = f"ğŸ‘¥ **í¸ì§‘ì ê²€í† **\n\n{content}\n\nìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ìŠ¹ì¸/ê±°ë¶€/ìˆ˜ì •)"
                elif interaction_type == "outline_review":
                    interaction_message = f"ğŸ“‹ **ì•„ì›ƒë¼ì¸ ê²€í† **\n\n{content}\n\nì•„ì›ƒë¼ì¸ì„ ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ìŠ¹ì¸/ê±°ë¶€/ìˆ˜ì •)"
                elif interaction_type == "section_review":
                    interaction_message = f"ğŸ“ **ì„¹ì…˜ ê²€í† **\n\n{content}\n\nì„¹ì…˜ì„ ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ìŠ¹ì¸/ê±°ë¶€/ìˆ˜ì •)"
                else:
                    interaction_message = f"ğŸ”„ **{interaction_type} ê²€í† **\n\n{content}\n\nì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ìŠ¹ì¸/ê±°ë¶€)"
                
                conv_context.add_assistant_message(
                    content=interaction_message,
                    source="STORM_Interaction_Agent",
                    metadata={
                        "storm_interaction": True,
                        "interaction_type": interaction_type,
                        "interaction_id": interaction_id
                    }
                )
            
            # ì‘ë‹µ ëŒ€ê¸°
            return await self._wait_for_storm_interaction_response(session_id, interaction_id)
            
        except Exception as e:
            logger.error("Failed to handle conversational STORM interaction", error=str(e))
            return {"action": "approve", "feedback": "ì˜¤ë¥˜ë¡œ ì¸í•œ ìë™ ìŠ¹ì¸"}

    async def _wait_for_storm_interaction_response(self, session_id: str, interaction_id: str, timeout: int = 300):
        """
        STORM ìƒí˜¸ì‘ìš© ì‘ë‹µ ëŒ€ê¸°
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if session_id in self.storm_service.human_interactions:
                interaction = self.storm_service.human_interactions[session_id]
                if interaction.get("response"):
                    return interaction["response"]
            
            await asyncio.sleep(1)
        
        # íƒ€ì„ì•„ì›ƒ ì‹œ ìë™ ìŠ¹ì¸
        return {"action": "approve", "feedback": "íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ ìë™ ìŠ¹ì¸"}

    async def _process_storm_interaction_response(
        self,
        session_id: str,
        interaction: Dict[str, Any],
        message: str,
        conv_context
    ) -> Optional[Dict[str, Any]]:
        """
        STORM ìƒí˜¸ì‘ìš© ì‘ë‹µ ì²˜ë¦¬
        """
        try:
            # ì‚¬ìš©ì ì‘ë‹µ ë¶„ì„
            message_lower = message.lower().strip()
            
            if any(word in message_lower for word in ["ìŠ¹ì¸", "ok", "ì¢‹ë‹¤", "ì§„í–‰", "ì˜ˆ", "yes"]):
                action = "approve"
                feedback = "ìŠ¹ì¸"
            elif any(word in message_lower for word in ["ê±°ë¶€", "no", "ì•ˆëœë‹¤", "ì•„ë‹ˆë‹¤", "ì·¨ì†Œ"]):
                action = "reject"
                feedback = "ê±°ë¶€"
            elif any(word in message_lower for word in ["ìˆ˜ì •", "ë³€ê²½", "ê³ ì¹˜ë‹¤", "ë°”ê¾¸ë‹¤"]):
                action = "modify"
                feedback = message
            else:
                action = "approve"
                feedback = message
            
            # ì‘ë‹µ ì €ì¥
            interaction["response"] = {
                "action": action,
                "feedback": feedback,
                "timestamp": time.time()
            }
            
            # ì‘ë‹µ í™•ì¸ ë©”ì‹œì§€ ìƒì„±
            if action == "approve":
                response_message = f"âœ… **{interaction['type']} ìŠ¹ì¸ë¨**\n\nê³„ì† ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤..."
            elif action == "reject":
                response_message = f"âŒ **{interaction['type']} ê±°ë¶€ë¨**\n\në‹¤ì‹œ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤..."
            else:
                response_message = f"ğŸ”„ **{interaction['type']} ìˆ˜ì • ìš”ì²­**\n\ní”¼ë“œë°±: {feedback}\n\nìˆ˜ì • í›„ ë‹¤ì‹œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤..."
            
            # ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            conv_context.add_assistant_message(
                content=response_message,
                source="STORM_Interaction_Agent",
                metadata={
                    "storm_interaction_response": True,
                    "action": action,
                    "interaction_type": interaction["type"]
                }
            )
            
            return {
                "response": response_message,
                "session_id": session_id,
                "agent_name": "STORM_Interaction_Agent",
                "agent_type": "storm",
                "agent_description": "STORM ìƒí˜¸ì‘ìš© ì‘ë‹µ ì²˜ë¦¬",
                "confidence": 0.95,
                "tools_used": ["storm_interaction"],
                "execution_time": 0.0,
                "metadata": {
                    "storm_interaction_response": True,
                    "action": action,
                    "interaction_type": interaction["type"]
                }
            }
            
        except Exception as e:
            logger.error("Failed to process STORM interaction response", error=str(e))
            return None

    async def _process_with_conversation_manager(
        self,
        message: str,
        user_id: str,
        session_id: str,
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ConversationManager ì¤‘ì‹¬ì˜ ë©”ì‹œì§€ ì²˜ë¦¬
        """
        try:
            logger.info("ğŸ¯ Processing with ConversationManager-based system")
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
            conv_context = self.conversation_manager.get_or_create_context(session_id)
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            conv_context.add_user_message(
                content=message,
                source=user_id,
                metadata=context
            )
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
            conversation_history = conv_context.get_context_for_llm()
            
            # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ ë¡œê¹…
            summary = conv_context.get_conversation_summary()
            logger.info("Processing with ConversationManager",
                       session_id=session_id,
                       context_messages=f"{summary['user_messages']} users, {summary['assistant_messages']} assistants")
            
            # ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì²˜ë¦¬
            if asyncio.iscoroutinefunction(self.multi_agent_system.process_query):
                response = await self.multi_agent_system.process_query(
                    query=message,
                    user_id=user_id,
                    session_id=session_id,
                    conversation_history=conversation_history
                )
            else:
                response = self.multi_agent_system.process_query(
                    query=message,
                    user_id=user_id,
                    session_id=session_id,
                    conversation_history=conversation_history
                )
            
            # ì‘ë‹µ ì²˜ë¦¬
            if asyncio.iscoroutine(response):
                response = await response
            
            response_text = response if isinstance(response, str) else str(response)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "response": response_text,
                "session_id": session_id,
                "agent_name": "KTDS_ConversationManager_Agent",
                "agent_type": "conversation_orchestrator",
                "agent_description": "ConversationManager ê¸°ë°˜ í†µí•© ì—ì´ì „íŠ¸",
                "confidence": 0.9,
                "tools_used": ["conversation_context", "azure_sql_storage"],
                "execution_time": 0.0,
                "metadata": {
                    "conversation_manager": True,
                    "azure_sql_storage": summary.get("azure_sql_enabled", False),
                    "session_messages": summary.get("total_messages", 0),
                    "buffer_size": summary.get("buffer_size", 20)
                }
            }
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            conv_context.add_assistant_message(
                content=response_text,
                source="KTDS_ConversationManager_Agent",
                metadata=result.get("metadata", {})
            )
            
            logger.info("âœ… ConversationManager processing completed successfully")
            return result
            
        except Exception as e:
            logger.error("ConversationManager processing failed", error=str(e))
            raise

    async def process_message_stream(
        self,
        message: str,
        user_id: str,
        session_id: str,
        context: Optional[Dict[str, Any]] = None,
        agent_mode: str = "normal"
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬ (STORM ëª¨ë“œ ì§€ì› + ì‹¤ì‹œê°„ ë¡œê·¸)
        """
        try:
            logger.info("Processing message stream", 
                       user_id=user_id, 
                       session_id=session_id,
                       agent_mode=agent_mode)
            
            if not self.initialized:
                raise RuntimeError("Agent service not initialized")

            # ì‹œì‘ ë¡œê·¸ ì „ì†¡
            yield {
                "type": "log",
                "level": "info",
                "message": f"ğŸš€ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹œì‘ - ëª¨ë“œ: {agent_mode}",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "metadata": {"stage": "initialization", "agent_mode": agent_mode}
            }

            # STORM ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            if agent_mode == "storm":
                if not self.storm_service:
                    yield {
                        "type": "log",
                        "level": "error",
                        "message": "âŒ STORM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    raise ValueError("STORM service not available")
                
                yield {
                    "type": "log",
                    "level": "info",
                    "message": "ğŸŒ©ï¸ STORM ì—°êµ¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...",
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat(),
                    "metadata": {"stage": "storm_initialization"}
                }
                
                # ì»¨í…ìŠ¤íŠ¸ì—ì„œ STORM ì„¤ì • ì¶”ì¶œ
                storm_config = None
                enable_human_loop = False
                
                if context:
                    if "storm_config" in context:
                        from app.models.chat_models import StormConfig
                        storm_config = StormConfig(**context["storm_config"])
                    enable_human_loop = context.get("enable_human_loop", False)
                
                yield {
                    "type": "log",
                    "level": "info",
                    "message": f"âš™ï¸ STORM ì„¤ì • ì™„ë£Œ - ìƒí˜¸ì‘ìš© ëª¨ë“œ: {'í™œì„±í™”' if enable_human_loop else 'ë¹„í™œì„±í™”'}",
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat(),
                    "metadata": {
                        "stage": "storm_config",
                        "human_loop": enable_human_loop,
                        "config": storm_config.__dict__ if storm_config else None
                    }
                }
                
                # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œì¸ ê²½ìš° ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
                if enable_human_loop:
                    yield {
                        "type": "log",
                        "level": "info",
                        "message": "ğŸ”„ ìƒí˜¸ì‘ìš© STORM ì—°êµ¬ ì‹œì‘",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    async for chunk in self.storm_service.process_storm_interactive(
                        topic=message,
                        user_id=user_id,
                        session_id=session_id,
                        config=storm_config
                    ):
                        # STORM ì´ë²¤íŠ¸ë„ ë¡œê·¸ í˜•íƒœë¡œ ì „ì†¡
                        chunk_type = chunk.get("type", "storm_progress")
                        yield {
                            "type": "log",
                            "level": "info",
                            "message": f"ğŸŒ©ï¸ STORM: {chunk.get('content', '')}",
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat(),
                            "metadata": {
                                "stage": "storm_processing",
                                "storm_type": chunk_type,
                                "step": chunk.get("step", 0),
                                "total_steps": chunk.get("total_steps", 6)
                            }
                        }
                        yield chunk
                else:
                    # ì¼ë°˜ STORM ëª¨ë“œ ì²˜ë¦¬
                    yield {
                        "type": "log",
                        "level": "info",
                        "message": "ğŸ“ ì¼ë°˜ STORM ì—°êµ¬ ì§„í–‰ ì¤‘...",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    response = await self.storm_service.process_storm_research(
                        topic=message,
                        user_id=user_id,
                        session_id=session_id,
                        config=storm_config
                    )
                    
                    yield {
                        "type": "log",
                        "level": "success",
                        "message": "âœ… STORM ì—°êµ¬ ì™„ë£Œ",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {
                            "stage": "storm_complete",
                            "processing_time": response.get("execution_time", 0)
                        }
                    }
                    
                    # ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë³€í™˜
                    yield {
                        "type": "storm_complete",
                        "content": response.get("response", ""),
                        "session_id": session_id,
                        "metadata": response.get("metadata", {})
                    }
                return

            # ì¼ë°˜ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            yield {
                "type": "log",
                "level": "info",
                "message": "ğŸ¤– ì¼ë°˜ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹œì‘",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "metadata": {"stage": "normal_processing"}
            }
            
            # ì„¸ì…˜ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            if self.session_manager:
                yield {
                    "type": "log",
                    "level": "debug",
                    "message": "ğŸ’¾ ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ì¤‘...",
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat()
                }
                await self._save_user_message(session_id, user_id, message)

            # ConversationManager ì»¨í…ìŠ¤íŠ¸ í™•ì¸
            if self.conversation_manager:
                yield {
                    "type": "log",
                    "level": "info",
                    "message": "ğŸ” ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™•ì¸ ì¤‘...",
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat()
                }
                
                conv_context = self.conversation_manager.get_or_create_context(session_id)
                if conv_context:
                    summary = conv_context.get_conversation_summary()
                    yield {
                        "type": "log",
                        "level": "info",
                        "message": f"ğŸ“Š ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œë¨ - ë©”ì‹œì§€ ìˆ˜: {summary.get('total_messages', 0)}",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {
                            "stage": "context_loaded",
                            "message_count": summary.get('total_messages', 0),
                            "user_messages": summary.get('user_messages', 0),
                            "assistant_messages": summary.get('assistant_messages', 0)
                        }
                    }

            # ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì²˜ë¦¬
            if self.multi_agent_system:
                yield {
                    "type": "log",
                    "level": "info",
                    "message": "ğŸ¯ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘...",
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat(),
                    "metadata": {"stage": "multi_agent_processing"}
                }
                
                # ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì—¬ë¶€ í™•ì¸
                if hasattr(self.multi_agent_system, 'process_stream'):
                    yield {
                        "type": "log",
                        "level": "info",
                        "message": "ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì—ì´ì „íŠ¸ ì²˜ë¦¬",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    async for chunk in self.multi_agent_system.process_stream(
                        message, user_id, session_id, context
                    ):
                        yield chunk
                else:
                    yield {
                        "type": "log",
                        "level": "info",
                        "message": "ğŸ“ ì¼ë°˜ ëª¨ë“œë¡œ ì—ì´ì „íŠ¸ ì²˜ë¦¬",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    # ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ (ì‹œë®¬ë ˆì´ì…˜)
                    response = await self.process_message(message, user_id, session_id, context, agent_mode)
                    
                    yield {
                        "type": "log",
                        "level": "success",
                        "message": f"âœ… ì—ì´ì „íŠ¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ - ê¸¸ì´: {len(response.get('response', ''))}ì",
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {
                            "stage": "response_generated",
                            "response_length": len(response.get('response', '')),
                            "agent_type": response.get('agent_type', 'orchestrator')
                        }
                    }
                    
                    # ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°
                    response_text = response.get("response", "")
                    if response_text:
                        words = response_text.split()
                        
                        yield {
                            "type": "log",
                            "level": "info",
                            "message": f"ğŸ“¤ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - {len(words)}ê°œ ë‹¨ì–´",
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        
                        for i, word in enumerate(words):
                            await asyncio.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ (ë” ë¹ ë¥´ê²Œ)
                            yield {
                                "type": "token",
                                "content": word + " ",
                                "agent_type": response.get("agent_type", "orchestrator"),
                                "session_id": session_id,
                                "is_final": i == len(words) - 1,
                                "progress": (i + 1) / len(words)
                            }
                            
                            # 10ë‹¨ì–´ë§ˆë‹¤ ì§„í–‰ ë¡œê·¸
                            if (i + 1) % 10 == 0:
                                yield {
                                    "type": "log",
                                    "level": "debug",
                                    "message": f"ğŸ“¤ ì§„í–‰ë¥ : {((i + 1) / len(words) * 100):.1f}%",
                                    "session_id": session_id,
                                    "timestamp": datetime.now().isoformat()
                                }
                        
                        yield {
                            "type": "log",
                            "level": "success",
                            "message": "ğŸ‰ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ",
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
            else:
                yield {
                    "type": "log",
                    "level": "warning",
                    "message": "âš ï¸ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat()
                }
                
                # ê°„ë‹¨í•œ ì‘ë‹µ ì œê³µ
                simple_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì‹œì§€: {message}"
                yield {
                    "type": "token",
                    "content": simple_response,
                    "agent_type": "fallback",
                    "session_id": session_id,
                    "is_final": True
                }
                    
        except Exception as e:
            logger.error("Error in stream processing", 
                        error=str(e),
                        session_id=session_id,
                        agent_mode=agent_mode)
            
            yield {
                "type": "log",
                "level": "error",
                "message": f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "metadata": {"stage": "error", "error": str(e)}
            }
            
            yield {
                "type": "error",
                "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
                "agent_type": "error",
                "session_id": session_id
            }

    async def get_conversation_history(self, session_id: str) -> List[Dict[str, Any]]:
        """ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (ConversationManager ì¤‘ì‹¬)"""
        try:
            if self.conversation_manager:
                # ConversationManagerì—ì„œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ (Azure SQL í¬í•¨)
                return self.conversation_manager.get_conversation_history(session_id)
            else:
                return []
        except Exception as e:
            logger.error("Failed to get conversation history", 
                        error=str(e),
                        session_id=session_id)
            return []

    async def clear_session(self, session_id: str):
        """ì„¸ì…˜ ì‚­ì œ (ConversationManager ì¤‘ì‹¬)"""
        try:
            if self.conversation_manager:
                # ConversationManagerì—ì„œ ì„¸ì…˜ ì •ë¦¬ (Azure SQL í¬í•¨)
                self.conversation_manager.clear_session(session_id)
                logger.info("Session cleared via ConversationManager", session_id=session_id)
            
            # ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ SessionManagerë„ ì •ë¦¬
            if self.session_manager and hasattr(self.session_manager, 'end_session'):
                self.session_manager.end_session(session_id)
                logger.info("Session cleared via SessionManager (compatibility)", session_id=session_id)
                
        except Exception as e:
            logger.error("Failed to clear session", 
                        error=str(e),
                        session_id=session_id)
            raise

    async def get_session_stats(self) -> Dict[str, Any]:
        """í†µí•© ì„¸ì…˜ í†µê³„ (ConversationManager ì¤‘ì‹¬ + STORM ì§€ì›)"""
        try:
            stats = {
                "conversation_manager": {},
                "session_manager": {},
                "storm_service": {},
                "service_status": {
                    "initialized": self.initialized,
                    "backend_available": BACKEND_AVAILABLE,
                    "agent_count": self.agent_count,
                    "storm_available": self.storm_service is not None
                }
            }
            
            # ConversationManager í†µê³„ (ë©”ì¸)
            if self.conversation_manager:
                stats["conversation_manager"] = self.conversation_manager.get_session_stats()
            
            # SessionManager í†µê³„ (í˜¸í™˜ì„±)
            if self.session_manager:
                stats["session_manager"] = self.session_manager.get_session_stats()
            
            # STORM ì„œë¹„ìŠ¤ í†µê³„
            if self.storm_service:
                stats["storm_service"] = await self.storm_service.get_storm_capabilities()
            
            return stats
            
        except Exception as e:
            logger.error("Failed to get session stats", error=str(e))
            return {"error": str(e)}

    async def get_user_chat_stats(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ìë³„ ì±„íŒ… í†µê³„"""
        try:
            # ConversationManager ê¸°ë°˜ í†µê³„
            if self.conversation_manager:
                all_sessions = self.conversation_manager.get_active_sessions()
                user_sessions = []
                total_messages = 0
                storm_usage = 0
                
                for session_id in all_sessions:
                    history = self.conversation_manager.get_conversation_history(session_id)
                    if history:
                        # ì‚¬ìš©ì ë©”ì‹œì§€ í™•ì¸
                        user_found = False
                        for msg in history:
                            if isinstance(msg, dict):
                                source = msg.get("source", "")
                                metadata = msg.get("metadata", {})
                            else:
                                source = getattr(msg, 'source', '')
                                metadata = getattr(msg, 'metadata', {})
                            
                            if source == user_id:
                                user_found = True
                                break
                        
                        if user_found:
                            user_sessions.append(session_id)
                            total_messages += len(history)
                            
                            # STORM ì‚¬ìš© í†µê³„ (ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸)
                            for msg in history:
                                if isinstance(msg, dict):
                                    metadata = msg.get("metadata", {})
                                else:
                                    metadata = getattr(msg, 'metadata', {})
                                
                                if metadata.get("storm_research", False):
                                    storm_usage += 1
                
                return {
                    "user_id": user_id,
                    "total_sessions": len(user_sessions),
                    "active_sessions": len(user_sessions),
                    "total_messages": total_messages,
                    "average_response_time": 1.5,  # ê¸°ë³¸ê°’
                    "agent_usage": {
                        "conversation_manager": len(user_sessions),
                        "storm": storm_usage,
                        "storm_interactive": 0  # ì¶”í›„ êµ¬í˜„
                    },
                    "success_rate": 0.95,
                    "popular_queries": [],
                    "last_activity": datetime.now() if user_sessions else None,
                    "most_active_hour": 14  # ê¸°ë³¸ê°’
                }
            else:
                return {"user_id": user_id, "total_sessions": 0, "total_messages": 0}
                
        except Exception as e:
            logger.error("Failed to get user chat stats", error=str(e), user_id=user_id)
            return {"user_id": user_id, "error": str(e)}

    async def cleanup(self):
        """ì„œë¹„ìŠ¤ ì •ë¦¬"""
        try:
            if self.conversation_manager:
                # ConversationManager ì •ë¦¬
                self.conversation_manager.cleanup_inactive_sessions()
            
            if self.session_manager:
                # SessionManager ì •ë¦¬ (í˜¸í™˜ì„±)
                if hasattr(self.session_manager, 'cleanup'):
                    await self.session_manager.cleanup()
            
            if self.storm_service:
                # STORM ì„œë¹„ìŠ¤ ì •ë¦¬
                await self.storm_service.cleanup()
            
            if self.multi_agent_system:
                # ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì •ë¦¬
                if hasattr(self.multi_agent_system, 'cleanup'):
                    await self.multi_agent_system.cleanup()
            
            logger.info("Agent Service cleanup completed")
            
        except Exception as e:
            logger.error("Error during service cleanup", error=str(e))

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€...
    async def analyze_query_complexity(self, message: str) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„
        """
        try:
            if self.multi_agent_system and hasattr(self.multi_agent_system, 'analyze_complexity'):
                return await self.multi_agent_system.analyze_complexity(message)
            else:
                # ê¸°ë³¸ ë³µì¡ë„ ë¶„ì„ êµ¬í˜„
                return await self._basic_complexity_analysis(message)
        except Exception as e:
            logger.error("Failed to analyze query complexity", 
                        error=str(e))
            return {
                "sub_queries": [message],
                "agents_involved": ["orchestrator"],
                "complexity_score": 0.5
            }
    
    async def submit_feedback(self, session_id: str, rating: int, comment: str):
        """
        í”¼ë“œë°± ì œì¶œ
        """
        try:
            if self.session_manager and hasattr(self.session_manager, 'submit_feedback'):
                await self.session_manager.submit_feedback(session_id, rating, comment)
            logger.info("Feedback submitted", 
                       session_id=session_id, 
                       rating=rating)
        except Exception as e:
            logger.error("Failed to submit feedback", 
                        error=str(e),
                        session_id=session_id)
            raise
    
    async def update_chat_stats(
        self,
        user_id: str,
        session_id: str,
        processing_time: float,
        success: bool
    ):
        """
        ì±„íŒ… í†µê³„ ì—…ë°ì´íŠ¸
        """
        try:
            if self.session_manager and hasattr(self.session_manager, 'update_stats'):
                await self.session_manager.update_stats(
                    user_id, session_id, processing_time, success
                )
        except Exception as e:
            logger.warning("Failed to update chat stats", 
                          error=str(e),
                          user_id=user_id)
    
    # Private methods
    
    async def _basic_complexity_analysis(self, message: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ë³µì¡ë„ ë¶„ì„"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë¶„ì„
        word_count = len(message.split())
        complexity_score = min(word_count / 50.0, 1.0)
        
        return {
            "sub_queries": [message],
            "agents_involved": ["orchestrator"],
            "complexity_score": complexity_score
        }

    async def _save_user_message(
        self, 
        session_id: str, 
        user_id: str, 
        message: str
    ):
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥"""
        try:
            if self.session_manager and hasattr(self.session_manager, 'save_message'):
                await self.session_manager.save_message(
                    session_id=session_id,
                    user_id=user_id,
                    message=message,
                    role="user"
                )
        except Exception as e:
            logger.warning("Failed to save user message", 
                          error=str(e),
                          session_id=session_id)

    async def _save_agent_message(
        self, 
        session_id: str, 
        response: str, 
        agent_type: str,
        metadata: Dict[str, Any]
    ):
        """ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ì €ì¥"""
        try:
            if self.session_manager and hasattr(self.session_manager, 'save_message'):
                await self.session_manager.save_message(
                    session_id=session_id,
                    user_id="system",
                    message=response,
                    role="assistant",
                    agent_type=agent_type,
                    metadata=metadata
                )
        except Exception as e:
            logger.warning("Failed to save agent message", 
                          error=str(e),
                          session_id=session_id)

    async def _create_fallback_response(
        self, 
        message: str, 
        error: str
    ) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "agent_name": "Fallback_Agent",
            "agent_type": "fallback",
            "agent_description": "í´ë°± ì—ì´ì „íŠ¸",
            "confidence": 0.1,
            "tools_used": [],
            "execution_time": 0.0,
            "metadata": {
                "error": error,
                "fallback": True
            }
        }

    async def _create_default_response(self, message: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        return {
            "response": f"ì•ˆë…•í•˜ì„¸ìš”! '{message}'ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "agent_name": "Default_Agent",
            "agent_type": "default",
            "agent_description": "ê¸°ë³¸ ì—ì´ì „íŠ¸",
            "confidence": 0.5,
            "tools_used": [],
            "execution_time": 0.0,
            "metadata": {
                "default_response": True
            }
        } 

    def _extract_topic_from_message(self, message: str) -> str:
        """ë©”ì‹œì§€ì—ì„œ ì—°êµ¬ ì£¼ì œ ì¶”ì¶œ"""
        # ì—°êµ¬ í‚¤ì›Œë“œ ì œê±°í•˜ê³  ì£¼ì œ ì¶”ì¶œ
        remove_keywords = ["ì—°êµ¬í•´ì¤˜", "ì—°êµ¬í•˜ê³ ", "ì—°êµ¬ í•´ì¤˜", "ë¦¬ì„œì¹˜", "ì¡°ì‚¬í•´ì¤˜", "ë¶„ì„í•´ì¤˜", "ì •ë¦¬í•´ì¤˜", "ì¨ì¤˜", "ì‘ì„±í•´ì¤˜", "ì— ëŒ€í•´", "ì—ëŒ€í•´", "ì—ê´€í•´", "ì— ê´€í•´"]
        
        topic = message
        for keyword in remove_keywords:
            topic = topic.replace(keyword, "")
        
        # ì „ì²˜ë¦¬
        topic = topic.strip()
        topic = topic.replace("  ", " ")  # ì´ì¤‘ ê³µë°± ì œê±°
        
        return topic if topic else "ì•Œ ìˆ˜ ì—†ëŠ” ì£¼ì œ"
    
    def _extract_storm_topic_from_history(self, messages: List[Dict[str, Any]]) -> Optional[str]:
        """ëŒ€í™” ê¸°ë¡ì—ì„œ STORM ì£¼ì œ ì¶”ì¶œ"""
        for msg in reversed(messages):
            if msg.get("role") == "assistant":
                metadata = msg.get("metadata", {})
                if metadata.get("storm_setup_question") and "storm_topic" in metadata:
                    return metadata["storm_topic"]
        return None